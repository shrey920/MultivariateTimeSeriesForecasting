{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import math\n",
    "import time\n",
    "import numpy as np;\n",
    "import importlib\n",
    "\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTNet(nn.Module):\n",
    "    def __init__(self, args, data):\n",
    "        super(LSTNet, self).__init__()\n",
    "        self.P = args.window;\n",
    "        self.m = data.m\n",
    "        self.hidR = args.hidRNN;\n",
    "        self.hidC = args.hidCNN;\n",
    "        self.hidS = args.hidSkip;\n",
    "        self.Ck = args.CNN_kernel;\n",
    "        self.skip = args.skip;\n",
    "        self.pt = int((self.P - self.Ck)/self.skip)\n",
    "        self.hw = args.highway_window\n",
    "        self.conv1 = nn.Conv2d(1, self.hidC, kernel_size = (self.Ck, self.m));\n",
    "        self.GRU1 = nn.GRU(self.hidC, self.hidR);\n",
    "        self.dropout = nn.Dropout(p = args.dropout);\n",
    "        if (self.skip > 0):\n",
    "            self.GRUskip = nn.GRU(self.hidC, self.hidS);\n",
    "            self.linear1 = nn.Linear(self.hidR + self.skip * self.hidS, self.m);\n",
    "        else:\n",
    "            self.linear1 = nn.Linear(self.hidR, self.m);\n",
    "        self.output = None;\n",
    "        if (args.output_fun == 'sigmoid'):\n",
    "            self.output = F.sigmoid;\n",
    "        if (args.output_fun == 'tanh'):\n",
    "            self.output = F.tanh;\n",
    " \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0);\n",
    "        \n",
    "        #CNN\n",
    "        c = x.view(-1, 1, self.P, self.m);\n",
    "        c = F.relu(self.conv1(c));\n",
    "        c = self.dropout(c);\n",
    "        c = torch.squeeze(c, 3);\n",
    "        \n",
    "        # RNN \n",
    "        r = c.permute(2, 0, 1).contiguous();\n",
    "        _, r = self.GRU1(r);\n",
    "        r = self.dropout(torch.squeeze(r,0));\n",
    "\n",
    "        \n",
    "        #skip-rnn\n",
    "        \n",
    "        if (self.skip > 0):\n",
    "            s = c[:,:, int(-self.pt * self.skip):].contiguous();\n",
    "            s = s.view(batch_size, self.hidC, self.pt, self.skip);\n",
    "            s = s.permute(2,0,3,1).contiguous();\n",
    "            s = s.view(self.pt, batch_size * self.skip, self.hidC);\n",
    "            _, s = self.GRUskip(s);\n",
    "            s = s.view(batch_size, self.skip * self.hidS);\n",
    "            s = self.dropout(s);\n",
    "            r = torch.cat((r,s),1);\n",
    "        \n",
    "        res = self.linear1(r);\n",
    "        \n",
    "            \n",
    "        if (self.output):\n",
    "            res = self.output(res);\n",
    "        return res;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_std(x):\n",
    "    return x.std() * np.sqrt((len(x) - 1.)/(len(x)))\n",
    "\n",
    "class Data_utility(object):\n",
    "    # train and valid is the ratio of training set and validation set. test = 1 - train - valid\n",
    "    def __init__(self, file_name, train, valid,  horizon, window, normalize = 2):\n",
    "#         self.cuda = cuda;\n",
    "        self.P = window\n",
    "        self.h = horizon\n",
    "        fin = open(file_name)\n",
    "        self.rawdat = np.loadtxt(fin,delimiter=',')\n",
    "        self.dat = np.zeros(self.rawdat.shape)\n",
    "        self.n, self.m = self.dat.shape\n",
    "        self.normalize = 2\n",
    "        self.scale = np.ones(self.m)\n",
    "        self._normalized(normalize)\n",
    "        self._split(int(train * self.n), int((train+valid) * self.n), self.n)\n",
    "        self.scale = torch.from_numpy(self.scale).float()\n",
    "        tmp = self.test[1] * self.scale.expand(self.test[1].size(0), self.m)\n",
    "\n",
    "        self.rse = normal_std(tmp)\n",
    "        self.rae = torch.mean(torch.abs(tmp - torch.mean(tmp)))\n",
    "    \n",
    "    def _normalized(self, normalize):\n",
    "        #normalized by the maximum value of entire matrix.\n",
    "       \n",
    "        if (normalize == 0):\n",
    "            self.dat = self.rawdat\n",
    "            \n",
    "        if (normalize == 1):\n",
    "            self.dat = self.rawdat / np.max(self.rawdat)\n",
    "            \n",
    "        #normlized by the maximum value of each row(sensor).\n",
    "        if (normalize == 2):\n",
    "            for i in range(self.m):\n",
    "                self.scale[i] = np.max(np.abs(self.rawdat[:,i]))\n",
    "                self.dat[:,i] = self.rawdat[:,i] / np.max(np.abs(self.rawdat[:,i]))\n",
    "            \n",
    "        \n",
    "    def _split(self, train, valid, test):\n",
    "        \n",
    "        train_set = range(self.P+self.h-1, train)\n",
    "        valid_set = range(train, valid)\n",
    "        test_set = range(valid, self.n)\n",
    "        self.train = self._batchify(train_set, self.h)\n",
    "        self.valid = self._batchify(valid_set, self.h)\n",
    "        self.test = self._batchify(test_set, self.h)\n",
    "        \n",
    "        \n",
    "    def _batchify(self, idx_set, horizon):\n",
    "        \n",
    "        n = len(idx_set)\n",
    "        X = torch.zeros((n,self.P,self.m))\n",
    "        Y = torch.zeros((n,self.m))\n",
    "        \n",
    "        for i in range(n):\n",
    "            end = idx_set[i] - self.h + 1\n",
    "            start = end - self.P\n",
    "            X[i,:,:] = torch.from_numpy(self.dat[start:end, :])\n",
    "            Y[i,:] = torch.from_numpy(self.dat[idx_set[i], :])\n",
    "\n",
    "        return [X, Y]\n",
    "\n",
    "    def get_batches(self, inputs, targets, batch_size, shuffle=True):\n",
    "        length = len(inputs)\n",
    "        if shuffle:\n",
    "            index = torch.randperm(length)\n",
    "        else:\n",
    "            index = torch.LongTensor(range(length))\n",
    "        start_idx = 0\n",
    "\n",
    "        while (start_idx < length):\n",
    "            end_idx = min(length, start_idx + batch_size)\n",
    "            excerpt = index[start_idx:end_idx]\n",
    "            X = inputs[excerpt]; Y = targets[excerpt]\n",
    "#             if (self.cuda):\n",
    "#                 X = X.cuda()\n",
    "#                 Y = Y.cuda()  \n",
    "            yield Variable(X), Variable(Y)\n",
    "            start_idx += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, X, Y, model, evaluateL2, evaluateL1, batch_size):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_loss_l1 = 0\n",
    "    n_samples = 0\n",
    "    predict = None\n",
    "    test = None\n",
    "    \n",
    "    for X, Y in data.get_batches(X, Y, batch_size, False):\n",
    "        output = model(X)\n",
    "        if predict is None:\n",
    "            predict = output\n",
    "            test = Y\n",
    "        else:\n",
    "            predict = torch.cat((predict,output))\n",
    "            test = torch.cat((test, Y))\n",
    "        \n",
    "        scale = data.scale.expand(output.size(0), data.m)\n",
    "        total_loss += evaluateL2(output * scale, Y * scale).data\n",
    "        total_loss_l1 += evaluateL1(output * scale, Y * scale).data\n",
    "        n_samples += (output.size(0) * data.m)\n",
    "    rse = math.sqrt(total_loss / n_samples)/data.rse\n",
    "    rae = (total_loss_l1/n_samples)/data.rae\n",
    "    \n",
    "    predict = predict.data.cpu().numpy()\n",
    "    Ytest = test.data.cpu().numpy()\n",
    "    sigma_p = (predict).std(axis = 0)\n",
    "    sigma_g = (Ytest).std(axis = 0)\n",
    "    mean_p = predict.mean(axis = 0)\n",
    "    mean_g = Ytest.mean(axis = 0)\n",
    "    index = (sigma_g!=0)\n",
    "    correlation = ((predict - mean_p) * (Ytest - mean_g)).mean(axis = 0)/(sigma_p * sigma_g)\n",
    "    correlation = (correlation[index]).mean()\n",
    "    return rse, rae, correlation\n",
    "\n",
    "def train(data, X, Y, model, criterion, batch_size):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    n_samples = 0\n",
    "    for X, Y in data.get_batches(X, Y, batch_size, False):\n",
    "        model.zero_grad()\n",
    "        output = model(X)\n",
    "        scale = data.scale.expand(output.size(0), data.m)\n",
    "        loss = criterion(output * scale, Y * scale)\n",
    "        loss.backward()\n",
    "        grad_norm = optim.step()\n",
    "        total_loss += loss.data\n",
    "        n_samples += (output.size(0) * data.m)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class Optim(object):\n",
    "\n",
    "    def _makeOptimizer(self):\n",
    "        if self.method == 'sgd':\n",
    "            self.optimizer = optim.SGD(self.params, lr=self.lr)\n",
    "        elif self.method == 'adagrad':\n",
    "            self.optimizer = optim.Adagrad(self.params, lr=self.lr)\n",
    "        elif self.method == 'adadelta':\n",
    "            self.optimizer = optim.Adadelta(self.params, lr=self.lr)\n",
    "        elif self.method == 'adam':\n",
    "            self.optimizer = optim.Adam(self.params, lr=self.lr)\n",
    "        else:\n",
    "            raise RuntimeError(\"Invalid optim method: \" + self.method)\n",
    "\n",
    "    def __init__(self, params, method, lr, max_grad_norm, lr_decay=1, start_decay_at=None):\n",
    "        self.params = list(params)  # careful: params may be a generator\n",
    "        self.last_ppl = None\n",
    "        self.lr = lr\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.method = method\n",
    "        self.lr_decay = lr_decay\n",
    "        self.start_decay_at = start_decay_at\n",
    "        self.start_decay = False\n",
    "\n",
    "        self._makeOptimizer()\n",
    "\n",
    "    def step(self):\n",
    "        # Compute gradients norm.\n",
    "        grad_norm = 0\n",
    "        for param in self.params:\n",
    "            grad_norm += math.pow(param.grad.data.norm(), 2)\n",
    "\n",
    "        grad_norm = math.sqrt(grad_norm)\n",
    "        if grad_norm > 0:\n",
    "            shrinkage = self.max_grad_norm / grad_norm\n",
    "        else:\n",
    "            shrinkage = 1.\n",
    "\n",
    "        for param in self.params:\n",
    "            if shrinkage < 1:\n",
    "                param.grad.data.mul_(shrinkage)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        return grad_norm\n",
    "\n",
    "    # decay learning rate if val perf does not improve or we hit the start_decay_at limit\n",
    "    def updateLearningRate(self, ppl, epoch):\n",
    "        if self.start_decay_at is not None and epoch >= self.start_decay_at:\n",
    "            self.start_decay = True\n",
    "        if self.last_ppl is not None and ppl > self.last_ppl:\n",
    "            self.start_decay = True\n",
    "\n",
    "        if self.start_decay:\n",
    "            self.lr = self.lr * self.lr_decay\n",
    "            print(\"Decaying learning rate to %g\" % self.lr)\n",
    "        #only decay for one epoch\n",
    "        self.start_decay = False\n",
    "\n",
    "        self.last_ppl = ppl\n",
    "\n",
    "        self._makeOptimizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self,data,hidCNN=100,hidRNN=100,window=35,CNN_kernel=6,highway_window=24,clip=10,epochs=100,batch_size=128,dropout=0.2,seed=54321,save=\"save.pt\",optim=\"adam\",lr=0.001,horizon=12,skip=24,hidskip=5,L1loss=True,normalize=2,output_fun=\"sigmoid\"):\n",
    "        self.data=data\n",
    "        self.hidCNN=hidCNN\n",
    "        self.hidRNN=hidRNN\n",
    "        self.window=window\n",
    "        self.CNN_kernel=CNN_kernel\n",
    "        self.highway_window=highway_window\n",
    "        self.clip=clip\n",
    "        self.epochs=epochs\n",
    "        self.batch_size=batch_size\n",
    "        self.dropout=dropout\n",
    "        self.seed=seed\n",
    "        self.optim=optim\n",
    "        self.lr=lr\n",
    "        self.skip=skip\n",
    "        self.normalize=normalize\n",
    "        self.horizon=horizon\n",
    "        self.save=save\n",
    "        self.output_fun=output_fun\n",
    "        self.hidSkip=hidskip\n",
    "        self.L1Loss=L1loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=Arguments(horizon=24,hidCNN=50, hidRNN=50,L1loss=False,data=\"data/exchange_rate.txt\",save=\"save/exch.pt\",output_fun=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4558)\n"
     ]
    }
   ],
   "source": [
    "Data = Data_utility(args.data, 0.6, 0.2, args.horizon, args.window, args.normalize);\n",
    "print(Data.rse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* number of parameters: 19973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/368847BE88477AFF/6th sem/Course Projects/IR project/MultivariateTimeSeriesForecasting/venv/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "model = LSTNet(args, Data)\n",
    "nParams = sum([p.nelement() for p in model.parameters()])\n",
    "print('* number of parameters: %d' % nParams)\n",
    "\n",
    "if args.L1Loss:\n",
    "    criterion = nn.L1Loss(size_average=False);\n",
    "else:\n",
    "    criterion = nn.MSELoss(size_average=False);\n",
    "evaluateL2 = nn.MSELoss(size_average=False);\n",
    "evaluateL1 = nn.L1Loss(size_average=False)\n",
    "\n",
    "best_val = 0;\n",
    "\n",
    "optim = Optim(\n",
    "    model.parameters(), args.optim, args.lr, args.clip,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "| end of epoch   1 | time:  1.21s | train_loss 4200.3667 | valid rse 0.3185 | valid rae 0.2401 | valid corr  0.7199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/368847BE88477AFF/6th sem/Course Projects/IR project/MultivariateTimeSeriesForecasting/venv/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type LSTNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| end of epoch   2 | time:  1.30s | train_loss 756.4346 | valid rse 0.4721 | valid rae 0.3728 | valid corr  0.7262\n",
      "| end of epoch   3 | time:  1.28s | train_loss 676.8607 | valid rse 0.5807 | valid rae 0.4145 | valid corr  0.7348\n",
      "| end of epoch   4 | time:  1.24s | train_loss 661.3216 | valid rse 0.4328 | valid rae 0.3152 | valid corr  0.7492\n",
      "| end of epoch   5 | time:  1.20s | train_loss 518.3625 | valid rse 0.4311 | valid rae 0.3094 | valid corr  0.7474\n",
      "test rse 0.4977 | test rae 0.3873 | test corr 0.8518\n",
      "| end of epoch   6 | time:  1.31s | train_loss 496.0251 | valid rse 0.4510 | valid rae 0.3152 | valid corr  0.7346\n",
      "| end of epoch   7 | time:  1.21s | train_loss 516.0242 | valid rse 0.3816 | valid rae 0.2842 | valid corr  0.7330\n",
      "| end of epoch   8 | time:  1.18s | train_loss 434.2618 | valid rse 0.3831 | valid rae 0.2775 | valid corr  0.7245\n",
      "| end of epoch   9 | time:  1.26s | train_loss 483.4277 | valid rse 0.3541 | valid rae 0.2687 | valid corr  0.6983\n",
      "| end of epoch  10 | time:  1.26s | train_loss 403.5934 | valid rse 0.3957 | valid rae 0.2860 | valid corr  0.6800\n",
      "test rse 0.4649 | test rae 0.3589 | test corr 0.8162\n",
      "| end of epoch  11 | time:  1.44s | train_loss 457.1249 | valid rse 0.3800 | valid rae 0.2698 | valid corr  0.5778\n",
      "| end of epoch  12 | time:  1.38s | train_loss 445.4815 | valid rse 0.3975 | valid rae 0.2808 | valid corr  0.6011\n",
      "| end of epoch  13 | time:  1.54s | train_loss 497.1268 | valid rse 0.3834 | valid rae 0.2770 | valid corr  0.5049\n",
      "| end of epoch  14 | time:  1.83s | train_loss 409.7008 | valid rse 0.3907 | valid rae 0.2925 | valid corr  0.4191\n",
      "| end of epoch  15 | time:  1.46s | train_loss 428.5336 | valid rse 0.3885 | valid rae 0.2866 | valid corr  0.4546\n",
      "test rse 0.4683 | test rae 0.3788 | test corr 0.5436\n",
      "| end of epoch  16 | time:  1.52s | train_loss 403.8446 | valid rse 0.4178 | valid rae 0.2920 | valid corr  0.5043\n",
      "| end of epoch  17 | time:  1.28s | train_loss 501.3693 | valid rse 0.4079 | valid rae 0.2990 | valid corr  0.3882\n",
      "| end of epoch  18 | time:  1.28s | train_loss 495.8554 | valid rse 0.3916 | valid rae 0.2705 | valid corr  0.4454\n",
      "| end of epoch  19 | time:  1.34s | train_loss 580.3034 | valid rse 0.4214 | valid rae 0.2971 | valid corr  0.4661\n",
      "| end of epoch  20 | time:  1.29s | train_loss 494.2423 | valid rse 0.4181 | valid rae 0.2939 | valid corr  0.4494\n",
      "test rse 0.4847 | test rae 0.3706 | test corr 0.5741\n",
      "| end of epoch  21 | time:  1.29s | train_loss 565.6910 | valid rse 0.4142 | valid rae 0.3004 | valid corr  0.3177\n",
      "| end of epoch  22 | time:  1.07s | train_loss 468.8157 | valid rse 0.4166 | valid rae 0.3061 | valid corr  0.4239\n",
      "| end of epoch  23 | time:  1.13s | train_loss 465.3864 | valid rse 0.3814 | valid rae 0.2944 | valid corr  0.4303\n",
      "| end of epoch  24 | time:  1.19s | train_loss 453.6446 | valid rse 0.3463 | valid rae 0.2498 | valid corr  0.4862\n",
      "| end of epoch  25 | time:  1.23s | train_loss 477.6160 | valid rse 0.3812 | valid rae 0.2835 | valid corr  0.5041\n",
      "test rse 0.4501 | test rae 0.3623 | test corr 0.7214\n",
      "| end of epoch  26 | time:  1.10s | train_loss 545.4937 | valid rse 0.3782 | valid rae 0.3087 | valid corr  0.3090\n",
      "| end of epoch  27 | time:  1.22s | train_loss 431.4644 | valid rse 0.3941 | valid rae 0.2970 | valid corr  0.4726\n",
      "| end of epoch  28 | time:  1.28s | train_loss 586.5822 | valid rse 0.4107 | valid rae 0.2924 | valid corr  0.2812\n",
      "| end of epoch  29 | time:  1.20s | train_loss 696.2308 | valid rse 0.3887 | valid rae 0.3220 | valid corr  0.4689\n",
      "| end of epoch  30 | time:  1.28s | train_loss 579.3293 | valid rse 0.3823 | valid rae 0.3242 | valid corr  0.3720\n",
      "test rse 0.4424 | test rae 0.3692 | test corr 0.3732\n",
      "| end of epoch  31 | time:  1.41s | train_loss 820.3776 | valid rse 0.4726 | valid rae 0.4969 | valid corr  0.2953\n",
      "| end of epoch  32 | time:  1.40s | train_loss 407.7155 | valid rse 0.3578 | valid rae 0.2949 | valid corr  0.5109\n",
      "| end of epoch  33 | time:  1.36s | train_loss 430.2754 | valid rse 0.3584 | valid rae 0.3352 | valid corr  0.4592\n",
      "| end of epoch  34 | time:  1.35s | train_loss 1092.0403 | valid rse 0.5215 | valid rae 0.5490 | valid corr  0.4589\n",
      "| end of epoch  35 | time:  1.72s | train_loss 669.1654 | valid rse 0.4663 | valid rae 0.4857 | valid corr  0.4504\n",
      "test rse 0.4604 | test rae 0.4780 | test corr 0.3877\n",
      "| end of epoch  36 | time:  1.28s | train_loss 837.9161 | valid rse 0.5332 | valid rae 0.5611 | valid corr  0.3400\n",
      "| end of epoch  37 | time:  1.38s | train_loss 635.2878 | valid rse 0.3955 | valid rae 0.3919 | valid corr  0.4596\n",
      "| end of epoch  38 | time:  1.36s | train_loss 748.5784 | valid rse 0.4809 | valid rae 0.5020 | valid corr  0.4272\n",
      "| end of epoch  39 | time:  1.37s | train_loss 617.9290 | valid rse 0.4113 | valid rae 0.4096 | valid corr  0.4053\n",
      "| end of epoch  40 | time:  1.35s | train_loss 693.6246 | valid rse 0.4886 | valid rae 0.5128 | valid corr  0.4249\n",
      "test rse 0.4630 | test rae 0.4879 | test corr 0.4885\n",
      "| end of epoch  41 | time:  1.39s | train_loss 633.5121 | valid rse 0.4651 | valid rae 0.4808 | valid corr  0.4713\n",
      "| end of epoch  42 | time:  1.35s | train_loss 654.1864 | valid rse 0.5139 | valid rae 0.5384 | valid corr  0.4380\n",
      "| end of epoch  43 | time:  1.38s | train_loss 564.0941 | valid rse 0.4010 | valid rae 0.4003 | valid corr  0.4668\n",
      "| end of epoch  44 | time:  1.34s | train_loss 636.0839 | valid rse 0.4626 | valid rae 0.4803 | valid corr  0.5076\n",
      "| end of epoch  45 | time:  1.34s | train_loss 505.7814 | valid rse 0.3516 | valid rae 0.3478 | valid corr  0.3951\n",
      "test rse 0.3483 | test rae 0.3364 | test corr 0.4419\n",
      "| end of epoch  46 | time:  1.28s | train_loss 651.3693 | valid rse 0.4742 | valid rae 0.4955 | valid corr  0.4623\n",
      "| end of epoch  47 | time:  1.34s | train_loss 478.7267 | valid rse 0.3663 | valid rae 0.3677 | valid corr  0.3525\n",
      "| end of epoch  48 | time:  1.24s | train_loss 570.4949 | valid rse 0.4757 | valid rae 0.4927 | valid corr  0.4276\n",
      "| end of epoch  49 | time:  1.38s | train_loss 531.4456 | valid rse 0.3897 | valid rae 0.3816 | valid corr  0.3995\n",
      "| end of epoch  50 | time:  1.39s | train_loss 567.6901 | valid rse 0.4279 | valid rae 0.4387 | valid corr  0.5138\n",
      "test rse 0.4034 | test rae 0.4140 | test corr 0.5222\n",
      "| end of epoch  51 | time:  1.26s | train_loss 559.5337 | valid rse 0.4478 | valid rae 0.4624 | valid corr  0.4907\n",
      "| end of epoch  52 | time:  1.19s | train_loss 550.3896 | valid rse 0.4803 | valid rae 0.4990 | valid corr  0.4799\n",
      "| end of epoch  53 | time:  1.30s | train_loss 526.0491 | valid rse 0.4322 | valid rae 0.4438 | valid corr  0.5228\n",
      "| end of epoch  54 | time:  1.36s | train_loss 522.2394 | valid rse 0.4394 | valid rae 0.4516 | valid corr  0.5306\n",
      "| end of epoch  55 | time:  1.40s | train_loss 530.1161 | valid rse 0.4288 | valid rae 0.4372 | valid corr  0.5231\n",
      "test rse 0.4104 | test rae 0.4132 | test corr 0.5298\n",
      "| end of epoch  56 | time:  1.34s | train_loss 504.0100 | valid rse 0.4349 | valid rae 0.4414 | valid corr  0.4714\n",
      "| end of epoch  57 | time:  1.33s | train_loss 446.2202 | valid rse 0.3697 | valid rae 0.3653 | valid corr  0.4820\n",
      "| end of epoch  58 | time:  1.27s | train_loss 471.4951 | valid rse 0.3554 | valid rae 0.3533 | valid corr  0.4712\n",
      "| end of epoch  59 | time:  1.24s | train_loss 516.9551 | valid rse 0.4255 | valid rae 0.4347 | valid corr  0.5080\n",
      "| end of epoch  60 | time:  1.22s | train_loss 497.5156 | valid rse 0.4312 | valid rae 0.4395 | valid corr  0.4487\n",
      "test rse 0.4216 | test rae 0.4251 | test corr 0.5808\n",
      "| end of epoch  61 | time:  1.27s | train_loss 506.0178 | valid rse 0.4500 | valid rae 0.4618 | valid corr  0.4721\n",
      "| end of epoch  62 | time:  1.39s | train_loss 471.4827 | valid rse 0.3953 | valid rae 0.3989 | valid corr  0.5180\n",
      "| end of epoch  63 | time:  1.37s | train_loss 462.8260 | valid rse 0.3905 | valid rae 0.3913 | valid corr  0.5195\n",
      "| end of epoch  64 | time:  1.44s | train_loss 373.4809 | valid rse 0.3140 | valid rae 0.3083 | valid corr  0.4690\n",
      "| end of epoch  65 | time:  1.53s | train_loss 348.6278 | valid rse 0.3090 | valid rae 0.3192 | valid corr  0.4110\n",
      "test rse 0.3433 | test rae 0.3398 | test corr 0.3757\n",
      "| end of epoch  66 | time:  1.44s | train_loss 531.8177 | valid rse 0.4443 | valid rae 0.4683 | valid corr  0.4863\n",
      "| end of epoch  67 | time:  1.37s | train_loss 522.6402 | valid rse 0.4738 | valid rae 0.4858 | valid corr  0.4556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| end of epoch  68 | time:  1.34s | train_loss 416.8128 | valid rse 0.3358 | valid rae 0.3425 | valid corr  0.5285\n",
      "| end of epoch  69 | time:  1.40s | train_loss 424.3530 | valid rse 0.3067 | valid rae 0.3031 | valid corr  0.5457\n",
      "| end of epoch  70 | time:  1.34s | train_loss 303.6581 | valid rse 0.2976 | valid rae 0.3033 | valid corr  0.4852\n",
      "test rse 0.3351 | test rae 0.3271 | test corr 0.5924\n",
      "| end of epoch  71 | time:  1.39s | train_loss 415.3339 | valid rse 0.3636 | valid rae 0.3738 | valid corr  0.5454\n",
      "| end of epoch  72 | time:  1.29s | train_loss 517.2302 | valid rse 0.4180 | valid rae 0.4312 | valid corr  0.5686\n",
      "| end of epoch  73 | time:  1.35s | train_loss 434.1713 | valid rse 0.4341 | valid rae 0.4518 | valid corr  0.4945\n",
      "| end of epoch  74 | time:  1.29s | train_loss 353.8126 | valid rse 0.3196 | valid rae 0.3293 | valid corr  0.4694\n",
      "| end of epoch  75 | time:  1.33s | train_loss 297.5581 | valid rse 0.3082 | valid rae 0.3160 | valid corr  0.5137\n",
      "test rse 0.3263 | test rae 0.3181 | test corr 0.5751\n",
      "| end of epoch  76 | time:  1.22s | train_loss 251.5660 | valid rse 0.3175 | valid rae 0.3115 | valid corr  0.5874\n",
      "| end of epoch  77 | time:  1.41s | train_loss 488.7440 | valid rse 0.3990 | valid rae 0.4050 | valid corr  0.5117\n",
      "| end of epoch  78 | time:  1.34s | train_loss 346.6730 | valid rse 0.3218 | valid rae 0.3421 | valid corr  0.4773\n",
      "| end of epoch  79 | time:  1.42s | train_loss 515.9163 | valid rse 0.4591 | valid rae 0.4795 | valid corr  0.4817\n",
      "| end of epoch  80 | time:  1.40s | train_loss 363.5327 | valid rse 0.3545 | valid rae 0.3625 | valid corr  0.5567\n",
      "test rse 0.3507 | test rae 0.3382 | test corr 0.6167\n",
      "| end of epoch  81 | time:  1.42s | train_loss 416.7393 | valid rse 0.3525 | valid rae 0.3491 | valid corr  0.5165\n",
      "| end of epoch  82 | time:  1.31s | train_loss 316.8792 | valid rse 0.3419 | valid rae 0.3570 | valid corr  0.5091\n",
      "| end of epoch  83 | time:  1.35s | train_loss 347.5476 | valid rse 0.3516 | valid rae 0.3584 | valid corr  0.5039\n",
      "| end of epoch  84 | time:  1.34s | train_loss 293.8746 | valid rse 0.3234 | valid rae 0.3365 | valid corr  0.4952\n",
      "| end of epoch  85 | time:  1.34s | train_loss 317.6919 | valid rse 0.3241 | valid rae 0.3361 | valid corr  0.4891\n",
      "test rse 0.3383 | test rae 0.3308 | test corr 0.5626\n",
      "| end of epoch  86 | time:  1.30s | train_loss 268.1870 | valid rse 0.3139 | valid rae 0.3235 | valid corr  0.4848\n",
      "| end of epoch  87 | time:  1.21s | train_loss 428.4482 | valid rse 0.4018 | valid rae 0.4077 | valid corr  0.4780\n",
      "| end of epoch  88 | time:  1.38s | train_loss 406.3232 | valid rse 0.3952 | valid rae 0.4034 | valid corr  0.4821\n",
      "| end of epoch  89 | time:  1.40s | train_loss 399.9965 | valid rse 0.3946 | valid rae 0.4043 | valid corr  0.4987\n",
      "| end of epoch  90 | time:  1.41s | train_loss 340.3434 | valid rse 0.3535 | valid rae 0.3612 | valid corr  0.5060\n",
      "test rse 0.3520 | test rae 0.3401 | test corr 0.6242\n",
      "| end of epoch  91 | time:  1.35s | train_loss 283.5242 | valid rse 0.3214 | valid rae 0.3319 | valid corr  0.4811\n",
      "| end of epoch  92 | time:  1.33s | train_loss 264.1005 | valid rse 0.3220 | valid rae 0.3307 | valid corr  0.4679\n",
      "| end of epoch  93 | time:  1.36s | train_loss 441.2386 | valid rse 0.4171 | valid rae 0.4252 | valid corr  0.3834\n",
      "| end of epoch  94 | time:  1.19s | train_loss 411.1189 | valid rse 0.4388 | valid rae 0.4519 | valid corr  0.4618\n",
      "| end of epoch  95 | time:  1.19s | train_loss 358.3109 | valid rse 0.3886 | valid rae 0.3977 | valid corr  0.5008\n",
      "test rse 0.3681 | test rae 0.3647 | test corr 0.6263\n",
      "| end of epoch  96 | time:  1.33s | train_loss 308.4797 | valid rse 0.3148 | valid rae 0.3178 | valid corr  0.4713\n",
      "| end of epoch  97 | time:  1.42s | train_loss 216.0176 | valid rse 0.3220 | valid rae 0.3290 | valid corr  0.4797\n",
      "| end of epoch  98 | time:  1.33s | train_loss 436.5808 | valid rse 0.4218 | valid rae 0.4243 | valid corr  0.3824\n",
      "| end of epoch  99 | time:  1.20s | train_loss 398.4034 | valid rse 0.4464 | valid rae 0.4571 | valid corr  0.4308\n",
      "| end of epoch 100 | time:  1.32s | train_loss 347.1099 | valid rse 0.3824 | valid rae 0.3926 | valid corr  0.4798\n",
      "test rse 0.3648 | test rae 0.3602 | test corr 0.6185\n",
      "\n",
      "\n",
      "\n",
      " After end of training.\n",
      "test rse 0.5028 | test rae 0.4048 | test corr 0.8518\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    print('begin training');\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_loss = train(Data, Data.train[0], Data.train[1], model, criterion, args.batch_size)\n",
    "        val_loss, val_rae, val_corr = evaluate(Data, Data.valid[0], Data.valid[1], model, evaluateL2, evaluateL1, args.batch_size);\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | train_loss {:5.4f} | valid rse {:5.4f} | valid rae {:5.4f} | valid corr  {:5.4f}'.format(epoch, (time.time() - epoch_start_time), train_loss, val_loss, val_rae, val_corr))\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "\n",
    "        if val_corr > best_val:\n",
    "            with open(args.save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val = val_corr\n",
    "        if epoch % 5 == 0:\n",
    "            test_acc, test_rae, test_corr  = evaluate(Data, Data.test[0], Data.test[1], model, evaluateL2, evaluateL1, args.batch_size);\n",
    "            print (\"test rse {:5.4f} | test rae {:5.4f} | test corr {:5.4f}\".format(test_acc, test_rae, test_corr))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "# Load the best saved model.\n",
    "with open(args.save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "test_acc, test_rae, test_corr  = evaluate(Data, Data.test[0], Data.test[1], model, evaluateL2, evaluateL1, args.batch_size);\n",
    "\n",
    "print(\"\\n\\n\\n After end of training.\")\n",
    "print (\"test rse {:5.4f} | test rae {:5.4f} | test corr {:5.4f}\".format(test_acc, test_rae, test_corr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
